% Chapter 1
% !TeX spellcheck = en_US 
\chapter{Statistical Learning Theory} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 
\setcounter{chapter}{2}
%----------------------------------------------------------------------------------------
In the previous chapter we have observed the phenomenon of overfitting; a model
trained to minimize the empirical risk on a training dataset can still fail to generalize
well over unseen dataset. A fundamental question in statistical learning theory is how to design
hypothesis classes that do not overfit.  

We will see in this chapter that restricting the hypothesis classes can help
reduce the overfitting. First, we start by looking at finite hypothesis classes
and show that they do not overfit. This result will also motivate a notion of statistical learning, that of \emph{probably approximately correct
(PAC)} learning. However, finiteness of the hypothesis class is, indeed, a very
restricting condition. We will look at infinite classes that do not lead to
overfitting and then characterize PAC learnability. In particular, we will
define the so-called \emph{VC-dimension} of a hypothesis class and show that
this property is sufficient and necessary for PAC learnability.

The reader is referred to~\cite{Shalev:ML:2014} and~\cite{Shapire:ThML2019} for more details. 

We start this section by recalling some basic definitions of probability
measure theory.
\section{Probability Measure Theory}
As we have seen in \nameref{Chapter1}, training datasets are treated as random
variables. This makes the following tools from probability theory essential. 

Let $(\Omega,\mathcal{A}, \mathbb{P})$ be a probability measure space, where $\Omega \subseteq \mathbb{R}$. 
It is common to refer to any $A \in \mathcal{A}$ by an \emph{event}. An event
$A$ s.t. $\mathbb{P}(A)=1$ is said to happen \emph{almost surley.}

Important probability measures are those induced by measurable transformations in the following way. 
\begin{definition}[Push-forward Measure]
	Given two measurable spaces $(\Omega_1, \mathcal{A}_1)$, $(\Omega_2, \mathcal{A}_2)$ and a measurable mapping $h:\Omega_1 \to \Omega_2$ 
	the \emph{push-forward measure} of a measure $\mathbb{P}$ on $(\Omega_2, \mathcal{A}_2)$ is 
	$$
	h_\#\mathbb{P} (A) := \mathbb{P} (h^{-1}(A)) \quad  \forall A \in \mathcal{A}_2.
	$$
	The push-forward measure is sometimes denoted by $\mathbb{P} h^{-1}$.
\end{definition}

We look now at a special kind of measurable mappings and the measures they induce.
\begin{definition}[Real Random Variables and Distributions]
	\label{def:RV}
	\begin{enumerate}[(i)] Let $(\Omega,\mathcal{A}, \mathbb{P})$ be a probability measure space.
		\item A measurable mapping $V: \Omega \to \mathbb{R}$ is called a \emph{real random variable}.
		\item The push-forward measure $\mathcal{P}_V := V_\# \mathbb{P}$ induced by a real random variable $V$ 
		is called the \emph{(probability) distribution of $V$}.		
	\end{enumerate}
\end{definition}
A mapping $V: \Omega \to \mathbb{R}^n$ for $n>1$ is usually referred to as a
\emph{random vector}. In our treatment we will refer to $V$ by a random variable
for any $n\geq 1$. 
\begin{definition}[Expected Value]
	\begin{itemize}
		\item The expected value of a random variable $X: \Omega \to
		\mathbb{R}$, denoted by $\mathbb{E}[X]$
		is defined as:
		\begin{align*}
			\mathbb{E}[X] &:= \int_\Omega X(\omega) \ d\mathbb{P}(\omega) \\
			& = \int_\mathbb{R} x \ d\mathcal{P}_X(\omega),
		\end{align*}
		where $\mathcal{P}_X$ is the pushforward-measure of $X$.
		\item Similarly, the expected value of a measurable mapping $g:
		\mathbb{R}\to \mathbb{R}$ as a function of the random variable $X$ is given by 
		\begin{align*}
			\mathbb{E}[g(X)] &:= \int_\Omega g(X)(\omega) \ d\mathbb{P}(\omega) \\
			& = \int_\mathbb{R} g(x) \ d\mathcal{P}_X(\omega).
		\end{align*}
		Sometimes, one writes $\mathbb{E}[g] = \mathbb{E}_{x \sim \mathcal{P}_x}[g]$
		to highlight the measure on $\mathbb{R}$ against which one integrates.
	\end{itemize}
\end{definition}


\begin{definition}[Independent Events]
Two events $A,B$ are said to be \emph{independent} if $$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B).$$ Given 
an index set $I$, consider the family $A_i \in \mathcal{A}$ for $i \in I$. The
family $(A_i)_{i\in I}$ of events is said to be \emph{independent}
if $$\mathbb{P}(\cap_{j \in J} A_j) = \prod_{j \in J} \mathbb{P}(A_j) \quad \forall J \subset I.$$	
\end{definition}

The independence of events (i.e., sets) can be generalized to independence of
families of sets. 
\begin{definition}[Independence of Families of Sets]
        Let $I$ be an index set and consider $\mathcal{E}_i \subseteq \mathcal{A}$ for all $i \in I$.
        The family $(\mathcal{E}_i)_{i \in I}$ is called \emph{independent} if, for any finite subset 
        $J \in I$ and any choice of $E_j \in \mathcal{E}_j, j \in J$, one has 
        $$
        \mathbb{P}(\cap_{j \in J} E_j) = \prod_{j \in J} \mathbb{P}(E_j).
        $$
\end{definition}
The following is an important family of random variables that one often encounters 
in machine learning and statistics. 
\begin{definition}[Independent and Identically Distributed Random Variables]
	\label{def:iid}
	Let $I$ be an index set and $(V_i)_{i \in I}$ be a family of real random 
	variables. Endow $\mathbb{R}$ with the Borel $\sigma$-algebra $\mathcal{B}$.
	\begin{enumerate}[(i)]
		\item The family $(V_i)_{i \in I}$ is said to be \emph{identically distributed} if 
		$$\mathcal{P}_{V_i} = \mathcal{P}_{V_j} \quad \forall i, j \in I.$$
		\item The family $(V_i)_{i \in I}$ is said to be \emph{independent} if the 
		family of generated $\sigma$-algebras $\bigl(\sigma(V_i) \bigr)_{i\in I}$, where 
		$\sigma(V_i) = V_i^{-1}(\mathcal{B})$ is independent.
	\end{enumerate}
A family of real random variables satisfying both conditions is said to be
\emph{independent and identically distributed (i.i.d.)}
In such a case set $\mathcal{P} = \mathcal{P}_{V_i}$. \end{definition}

\section{Probably Approximately Correct Learning}

We start this chapter by formalizing the setting of learning and the problem of
overfitting. We then look closely at the problem of overfitting and show that
finite classes do not overfit. Our results to this end motivate a notion of
statistical learning that we discuss.

\subsection{A Formal Setting of Learning}
Let $z = (x, y)$ be a random vector where $x: \Omega \to \mathbb{R}^n$ and $y$
is generated from $x$ by the functional relation $y=f(x)$, where $f: \mathbb{R}
\to \{0,1\}$, i.e., we restrict ourselves to the classification case. Denote by
$\mathcal{P}$ the probability distribution of $z$ and by $\mathcal{P}_x$ the
marginal probability distribution corresponding to the random vector $x$.
Further, we set $\mathbb{Z} := \mathbb{R}^n \times \{0,1\}$ \footnote{Do not
confuse the notation with the set of integers.}.

Given a hypothesis class $\mathfrak{H}$  and a loss function $l$ the goal of a supervised-learning
algorithm is to solve 
\begin{equation*}
    \underbrace{R_{\mathcal{P}}(h) := \int_{\mathbb{Z}} l(y, h(x)) \ d \mathcal{P}}_{\text{True risk}} \longrightarrow \min_{h \in \mathfrak{H}} \implies h^\star, 
\end{equation*}
that is, to minimize the \emph{true risk}. Note that the true risk is also called the
\emph{generalization error}. However, one only has access to a finite
realiazation of the random vector $z$, i.e., to a training set $D=\{(x_i,
y_i)_{i=1}^m\}$. A reasonable think to do is, hence, to minimize a
finite/empirical representation of the true risk, i.e., to solve
\begin{equation*}
    \underbrace{\hat{R}_{\mathcal{P}} (h) := \sum_{(x,y) \in D} \frac{1}{|D|}l(y, h(x)) }_{\text{Empirical risk}} \longrightarrow \min_{h \in \mathfrak{H}} \implies \hat{h}^\star.
\end{equation*}

A learning algorithm that replaces the original task of minimizing the true risk
by the task of minimizing the empirical risk is called an \emph{ERM learner} or
is said to be using the \emph{ERM learning} rule. 

One of the main problems of statistical learning theories is to study the
validity of approximating $h^*$ by $\hat{h}^*$. In particular, under what
conditions on the hypothesis class does $\hat{h}^*$ have a small generalization error?
In the next section, we will see that the finiteness of the hypothesis class is
a sufficient condition to this end. Is it necessarily though?


\subsection{Finite Hypothesis Classes do not Overfit}
The goal in this section is to show that we won't encounter an overfitting
problem if the hypothesis class has finitely many elements. However, our result
will be limited to the  0-1 loss which we define as follows.
\begin{equation*}
    l(y, h(x)) :=
    \begin{cases}
         & 1: \ h(x) \neq y \\
        & 0: \ \text{otherwise}.
    \end{cases}	
\end{equation*}
Note that in this case, the true and empirical risks simplify to
$$
R_{\mathcal{P}}(h) = \mathcal{P}_x \bigl(\{x: h(x) \neq y\} \bigr)
$$
$$
\hat{R}_{\mathcal{P}}(h) = \frac{1}{m} | \{x: h(x) \neq y\} |
$$

Moreover, our analysis will only work under the  
 \emph{Realizability Assumption}, i.e., that
there exists $h^* \in \mathfrak{H}$ s.t. $R_{\mathcal{P}_z}(h^*) = 0$.

What does that mean for training data? tba.
    \begin{lemma}
		Let $\mathfrak{H}$ be a finite hypothesis class, and $l$ be the 0-1 loss
		function. Let $S|_x = (x_1, \dots, x_m)$ be a set of $m$ training points
		sampled from $\mathcal{P}_x$. Under the realizability assumption and
		for accuracy $\epsilon >0$ it holds that 
		$$
		\mathcal{P}_x\bigl(\{S|_x: R_\mathcal{P}(h_s) > \epsilon\} \bigr) \leq |\mathfrak{H}| e ^{-\epsilon m}.
		$$
	\end{lemma}
	In other words: the probability of sampling $m$ training data points and
	obtaining a learner $h_s$ by the ERM rule that does not generalize well is
	upperbounded by a finite quantity.
\begin{proof}
    We start by defining the set $\mathfrak{H}_b$ of bad hypotheses, i.e., the
    set of all hypoehses that lead to a generalization error $> \epsilon$,
    $$
    \mathfrak{H}_b = \{h \in \mathfrak{H} \text{ s.t. } \mathbb{R}_\mathcal{P} (h) > \epsilon\}.
    $$
    Next, we define the set of misleading training data, i.e., the set of all
    training datasets of cardinality $m$, on which there is at least one
    hypothesis that produces zero training error and a generalization error $>
    \epsilon$,
    $$
    M:= \{D|_x: \ |D|_x|=m, \text{ and s.t. there exists }h  \in \mathfrak{H}_b : \ R_\mathcal{P}(h)=0 \}.
    $$
    Note that the realizability assumption implies that $R_\mathcal{P}(\hat{h}^*)$
\end{proof}
    \begin{coro}
		\label{Coro:finite_hypo}
		Let $\mathfrak{H}$ be a finite hypothesis class and $l$ be the 0-1 loss
		function. Let $\delta \in (0,1)$ be a confidence parameter and $\epsilon \in
		(0,1)$ be an accuracy parameter. Let $m$ be an integer that satisfies
		$$
		m \geq \frac{1}{\epsilon} \log(|\mathfrak{H}|/\delta).
		$$ 	
		Under the realizability assumption it holds that 
		$$
		\mathbb{P}_{S|_x \sim \mathcal{P}_x} \bigl( \{ R_\mathcal{P}(h_s) > \epsilon \}\bigr) \leq \delta.
		$$
		In other words, 
		$$
		R_\mathcal{P}(h_s) \leq \epsilon
		$$
		holds with probability of at least $1-\delta$ over the choice of the
		training data $S|x$.
	\end{coro}
    \begin{proof}
        tba.
    \end{proof}
\section{Probably Approximately Correct Learning}
    \begin{definition}
		A hypothesis class $\mathfrak{H}$ is PAC learnable if there exist a
		function $m_\mathfrak{H}: (0,1)^2 \to \mathbb{N}$ and a learning
		algorithm with the following properties
		\begin{itemize}
			\item for every $(\epsilon, \delta) \in (0,1)$
			\item for every distribution $\mathcal{P}_x$ over $\mathcal{X}$
			\item for every labeling function $f: \mathcal{X} \to \{0,1\}$
		\end{itemize}
		if the realizability assumption holds, when running the learning
		algorithm on $m \geq m_\mathfrak{H}(\delta, \epsilon)$ i.i.d. samples
		generated by $\mathcal{D}$ and labeled by $f$, the algorithm returns a
		hypothesis $h$ such that 
		$$
		R_\mathcal{P}(h) \leq \epsilon
		$$ 
		with probability of at least $1-\delta$ over the choice of the samples.
	\end{definition}

    \begin{definition}[samples complexity]
		The sample complexity of leaning a hypothesis class $\mathfrak{H}$ is
		the minimal integer that satisfies the requirement of PAC learnability
		with accuracy $\epsilon$ and confidence $\delta$.
	\end{definition}
	\begin{coro}
		Every finite hypothesis calss is PAC learnable with sample complexity 
		$$
		m \leq \lceil \frac{1}{\epsilon} \log(|\mathfrak{H}|/\delta) \rceil
		$$
	\end{coro}
Are there classes that are infinite but nevertheless PAC learnable? How to
characterize such classes?
\subsection{Balancing training and test errors}
$$
	R_{\mathcal{P}_z}(\hat{h}^*) = \underbrace{\min_{h \in \mathfrak{H}} R_{\mathcal{P}_z}(h)}_{\text{approximation/training error}} + \underbrace{R_{\mathcal{P}_z}(\hat{h}^*) - \min_{h \in \mathfrak{H}} R_{\mathcal{P}_z}(h)}_{\text{estimation/test error}} 
	$$

\section*{Wait! What is what?}
Here is a list of questions that help you check your understanding of key
concepts inside this chapter.

\begin{enumerate}
    \item tba. 
\end{enumerate}

